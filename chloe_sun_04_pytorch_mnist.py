# -*- coding: utf-8 -*-
"""Chloe Sun - 04-pytorch-mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-WSmv_xChVVh2OYxBaNOq3yfEkr3rej

[![AnalyticsDojo](https://github.com/rpi-techfundamentals/fall2018-materials/blob/master/fig/final-logo.png?raw=1)](http://rpi.analyticsdojo.com)
<center><h1>Pytorch with the MNIST Dataset - MINST</h1></center>
<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/04-pytorch-mnist.ipynb)

From Kaggle: 
"MNIST ("Modified National Institute of Standards and Technology") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike."

[Read more.](https://www.kaggle.com/c/digit-recognizer)


<a title="By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:MnistExamples.png"><img width="512" alt="MnistExamples" src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png"/></a>

This code is adopted from the pytorch examples repository. 
It is licensed under BSD 3-Clause "New" or "Revised" License.
Source: https://github.com/pytorch/examples/
LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE

![](https://github.com/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/mnist-comparison.png?raw=1)
Table from [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)
"""

!pip install torch torchvision

"""### Pytorch Advantages vs Tensorflow
- Pytorch Enables dynamic computational graphs (which change be changed) while Tensorflow is static. 
- Tensorflow enables easier deployment. 
"""

#Import Libraries


from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable
from PIL import Image,ImageOps

args={}
  kwargs={}
  args['batch_size']=1000
  args['test_batch_size']=1000
  args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. 
  args['lr']=0.01 #Learning rate is how fast it will decend. 
  args['momentum']=0.25 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).

  args['seed']=1 #random seed
  args['log_interval']=10
  args['cuda']=False

!wget https://figshare.com/ndownloader/files/13496366
!unzip 13496366
!mv histopathological\ image\ dataset\ for\ ET/NE/Follicular histopathological\ image\ dataset\ for\ ET/NE_Follicular
!mv histopathological\ image\ dataset\ for\ ET/NE/Luteal histopathological\ image\ dataset\ for\ ET/NE_Luteal
!mv histopathological\ image\ dataset\ for\ ET/NE/Menstrual histopathological\ image\ dataset\ for\ ET/NE_Menstrual
!mv histopathological\ image\ dataset\ for\ ET/NE/Simple histopathological\ image\ dataset\ for\ ET/EH_Simple
!mv histopathological\ image\ dataset\ for\ ET/NE/Complex histopathological\ image\ dataset\ for\ ET/EH_Complex
!rmdir histopathological\ image\ dataset\ for\ ET/NE
!rmdir histopathological\ image\ dataset\ for\ ET/EH

from torchvision.transforms.transforms import Grayscale
dataset = datasets.ImageFolder('histopathological image dataset for ET',
                               transform=transforms.Compose([transforms.ToTensor(),transforms.Resize([50,50])]))

dataset_train = torch.utils.data.Subset(dataset,[x for x in range(len(dataset)) if x%10 != 0])
dataset_test = torch.utils.data.Subset(dataset,[x for x in range(len(dataset)) if x%10 == 0])

train_loader = torch.utils.data.DataLoader(dataset_train,batch_size=32,shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset_test,batch_size=1000,shuffle=False)

import matplotlib.pyplot as plt

for data, target in train_loader:
  print(target)
  break
plt.imshow(data[0][0,:,:])

class Net(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.LazyConv2d(32, kernel_size=3)
        self.conv2 = nn.LazyConv2d(32, kernel_size=3)

        self.conv3 = nn.LazyConv2d(64, kernel_size=3)
        self.conv4 = nn.LazyConv2d(64, kernel_size=3)

        self.conv2_drop = nn.Dropout2d()  #Dropout
        #self.fc1 = nn.Linear(180, 50)
        #self.fc2 = nn.Linear(50, 10)

        self.fc1 = nn.LazyLinear(128)
        self.fc2 = nn.LazyLinear(7) #reduce the number of categories

    def forward(self, x):
        #Convolutional Layer/Pooling Layer/Activation
        x = F.relu(self.conv1(x)) 
        x = F.relu(self.conv2(x))

        x = F.max_pool2d(x,2)

        x = F.relu(self.conv3(x)) 
        x = F.relu(self.conv4(x))
        x = F.max_pool2d(x,2)

        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        #Softmax gets probabilities. 
        return F.log_softmax(x, dim=1)

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        #Variables in Pytorch are differenciable. 
        data, target = Variable(data), Variable(target)
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.
        loss = F.cross_entropy(output, target)
        #dloss/dx for every Variable 
        loss.backward()
        #to do a one-step update on our parameter.
        optimizer.step()
        #Print out the loss periodically. 
        if batch_idx % args['log_interval'] == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data))

def test(x):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)

        output = model(data)
        test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss | F.nll_loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

accuracy_over_time=[]

model = Net()
if args['cuda']:
    model.cuda()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.25)

for epoch in range(1, 50 + 1):
    train(epoch)
    acc = test(epoch)
    accuracy_over_time.append(acc)